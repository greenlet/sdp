{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import sys\n",
    "import shutil\n",
    "import traceback\n",
    "from typing import Union, Any, Optional\n",
    "import yaml\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.vision_transformer import _load_weights\n",
    "from timm.models.layers import DropPath \n",
    "\n",
    "from sdp.ds.bop_dataset import BopDataset, AUGNAME_DEFAULT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        trunc_normal_(m.weight, std=0.02)\n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "def resize_pos_embed(posemb, grid_old_shape, grid_new_shape, num_extra_tokens):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    posemb_tok, posemb_grid = (\n",
    "        posemb[:, :num_extra_tokens],\n",
    "        posemb[0, num_extra_tokens:],\n",
    "    )\n",
    "    if grid_old_shape is None:\n",
    "        gs_old_h = int(math.sqrt(len(posemb_grid)))\n",
    "        gs_old_w = gs_old_h\n",
    "    else:\n",
    "        gs_old_h, gs_old_w = grid_old_shape\n",
    "\n",
    "    gs_h, gs_w = grid_new_shape\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old_h, gs_old_w, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=(gs_h, gs_w), mode=\"bilinear\")\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_h * gs_w, -1)\n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    return posemb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size: tuple[int, int], patch_size: int, embed_dim: int, channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        if image_size[0] % patch_size != 0 or image_size[1] % patch_size != 0:\n",
    "            raise ValueError(\"image dimensions must be divisible by the patch size\")\n",
    "        self.grid_size = image_size[0] // patch_size, image_size[1] // patch_size\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            channels, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, im):\n",
    "        B, C, H, W = im.shape\n",
    "        x = self.proj(im).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, dropout_prob: float, out_dim: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        if out_dim is None:\n",
    "            out_dim = dim\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.drop = nn.Dropout(dropout_prob)\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        head_dim = dim // heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.attn = None\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, N, C = x.shape\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B, N, 3, self.heads, C // self.heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        q, k, v = (\n",
    "            qkv[0],\n",
    "            qkv[1],\n",
    "            qkv[2],\n",
    "        )\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_dim, dropout, drop_path):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, heads, dropout)\n",
    "        self.mlp = FeedForward(dim, mlp_dim, dropout)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x), mask)\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: tuple[int, int],\n",
    "        patch_size: int,\n",
    "        n_layers: int,\n",
    "        d_model: int,\n",
    "        d_ff: int,\n",
    "        n_heads: int,\n",
    "        n_cls: int,\n",
    "        dropout_prob: float = 0.1,\n",
    "        drop_path_rate: float = 0.0,\n",
    "        distilled: bool = False,\n",
    "        channels: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            image_size,\n",
    "            patch_size,\n",
    "            d_model,\n",
    "            channels,\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.n_cls = n_cls\n",
    "\n",
    "        # cls and pos tokens\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.distilled = distilled\n",
    "        if self.distilled:\n",
    "            self.dist_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.randn(1, self.patch_embed.num_patches + 2, d_model)\n",
    "            )\n",
    "            self.head_dist = nn.Linear(d_model, n_cls)\n",
    "        else:\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.randn(1, self.patch_embed.num_patches + 1, d_model)\n",
    "            )\n",
    "\n",
    "        # transformer blocks\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, n_layers)]\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(d_model, n_heads, d_ff, dropout_prob, dpr[i]) for i in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # output head\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, n_cls)\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=0.02)\n",
    "        trunc_normal_(self.cls_token, std=0.02)\n",
    "        if self.distilled:\n",
    "            trunc_normal_(self.dist_token, std=0.02)\n",
    "        self.pre_logits = nn.Identity()\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\"pos_embed\", \"cls_token\", \"dist_token\"}\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=\"\"):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "\n",
    "    def forward(self, im, return_features=False):\n",
    "        B, _, H, W = im.shape\n",
    "        PS = self.patch_size\n",
    "\n",
    "        x = self.patch_embed(im)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        if self.distilled:\n",
    "            dist_tokens = self.dist_token.expand(B, -1, -1)\n",
    "            x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        pos_embed = self.pos_embed\n",
    "        num_extra_tokens = 1 + self.distilled\n",
    "        if x.shape[1] != pos_embed.shape[1]:\n",
    "            pos_embed = resize_pos_embed(\n",
    "                pos_embed,\n",
    "                self.patch_embed.grid_size,\n",
    "                (H // PS, W // PS),\n",
    "                num_extra_tokens,\n",
    "            )\n",
    "        x = x + pos_embed\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if return_features:\n",
    "            return x\n",
    "\n",
    "        if self.distilled:\n",
    "            x, x_dist = x[:, 0], x[:, 1]\n",
    "            x = self.head(x)\n",
    "            x_dist = self.head_dist(x_dist)\n",
    "            x = (x + x_dist) / 2\n",
    "        else:\n",
    "            x = x[:, 0]\n",
    "            x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def get_attention_map(self, im, layer_id):\n",
    "        if layer_id >= self.n_layers or layer_id < 0:\n",
    "            raise ValueError(\n",
    "                f\"Provided layer_id: {layer_id} is not valid. 0 <= {layer_id} < {self.n_layers}.\"\n",
    "            )\n",
    "        B, _, H, W = im.shape\n",
    "        PS = self.patch_size\n",
    "\n",
    "        x = self.patch_embed(im)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        if self.distilled:\n",
    "            dist_tokens = self.dist_token.expand(B, -1, -1)\n",
    "            x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        pos_embed = self.pos_embed\n",
    "        num_extra_tokens = 1 + self.distilled\n",
    "        if x.shape[1] != pos_embed.shape[1]:\n",
    "            pos_embed = resize_pos_embed(\n",
    "                pos_embed,\n",
    "                self.patch_embed.grid_size,\n",
    "                (H // PS, W // PS),\n",
    "                num_extra_tokens,\n",
    "            )\n",
    "        x = x + pos_embed\n",
    "\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < layer_id:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                return blk(x, return_attention=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 384, 384\n",
    "patch_size = 16\n",
    "n_layers = 12\n",
    "d_model = 192\n",
    "mlp_expansion_ratio = 4\n",
    "d_ff = d_model * mlp_expansion_ratio\n",
    "n_heads = 3\n",
    "n_cls = 9\n",
    "dropout_prob = 0.1,\n",
    "drop_path_rate = 0.1\n",
    "distilled = False\n",
    "channels = 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embed = PatchEmbedding(\n",
    "    image_size,\n",
    "    patch_size,\n",
    "    d_model,\n",
    "    channels,\n",
    ")\n",
    "dp_layer = nn.Dropout(dropout_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1769472"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*32*9*192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3790, -0.8871, -0.6805, -0.6233],\n",
      "         [ 0.5185,  0.2006,  0.4642, -1.5103],\n",
      "         [ 0.6316,  0.2889, -1.1087,  1.6352]],\n",
      "\n",
      "        [[ 0.4064, -0.4138, -0.5595,  1.8089],\n",
      "         [ 0.2942, -0.5813, -1.9114,  0.6275],\n",
      "         [-0.4187, -0.0467,  0.3869,  0.6564]]])\n",
      "tensor([[[ 0.5061, -0.9613, -0.7218, -0.6556],\n",
      "         [ 0.6677,  0.2993,  0.6049, -1.6836],\n",
      "         [ 0.7988,  0.4016, -1.2181,  1.9620]],\n",
      "\n",
      "        [[ 0.4417, -0.4978, -0.6646,  2.0482],\n",
      "         [ 0.3132, -0.6896, -2.2132,  0.6950],\n",
      "         [-0.5033, -0.0772,  0.4194,  0.7281]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# NLP Example\n",
    "batch, sentence_length, embedding_dim = 2, 3, 4\n",
    "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    "# layer_norm = nn.LayerNorm(embedding_dim)\n",
    "layer_norm = nn.LayerNorm((sentence_length, embedding_dim))\n",
    "# Activate module\n",
    "print(embedding)\n",
    "print(layer_norm(embedding))\n",
    "# # Image Example\n",
    "# N, C, H, W = 20, 5, 10, 10\n",
    "# input = torch.randn(N, C, H, W)\n",
    "# # Normalize over the last three dimensions (i.e. the channel and spatial dimensions)\n",
    "# # as shown in the image below\n",
    "# layer_norm = nn.LayerNorm([C, H, W])\n",
    "# output = layer_norm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1], [2], [3]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (6) must match the existing size (3) at non-singleton dimension 0.  Target sizes: [6, 3].  Tensor sizes: [3, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/misha/prog/sdp/notebooks/s01_build.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/misha/prog/sdp/notebooks/s01_build.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x\u001b[39m.\u001b[39;49mexpand(\u001b[39m6\u001b[39;49m, \u001b[39m3\u001b[39;49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (6) must match the existing size (3) at non-singleton dimension 0.  Target sizes: [6, 3].  Tensor sizes: [3, 1]"
     ]
    }
   ],
   "source": [
    "x.expand(6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 1, 0, 1, 0, 0, 1],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [1, 0, 1, 0, 1, 1, 0, 1]], dtype=torch.uint8),\n",
       " 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(3, 8, dtype=torch.uint8)\n",
    "torch.bernoulli(x, 0.5), len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdp_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
