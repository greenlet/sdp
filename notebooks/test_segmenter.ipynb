{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/misha/prog/sdp/notebooks', '/Users/misha/prog/sdp', '/Users/misha/prog/lib/segmenter', '/Users/misha/miniconda3/envs/sdp_py310/lib/python310.zip', '/Users/misha/miniconda3/envs/sdp_py310/lib/python3.10', '/Users/misha/miniconda3/envs/sdp_py310/lib/python3.10/lib-dynload', '', '/Users/misha/miniconda3/envs/sdp_py310/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import sys\n",
    "\n",
    "# seg_path = '../../lib/segmenter'\n",
    "# if seg_path not in sys.path:\n",
    "#     sys.path.append(seg_path)\n",
    "# if '..' not in sys.path:\n",
    "#     sys.path.append('..')\n",
    "print(sys.path)\n",
    "\n",
    "import shutil\n",
    "import traceback\n",
    "from typing import Union, Any, Optional\n",
    "import yaml\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sdp.ds.bop_data import read_models_info, read_scene_camera, read_scene_gt, \\\n",
    "    read_scene_gt_info\n",
    "from sdp.ds.bop_dataset import BopDataset, AUGNAME_DEFAULT\n",
    "\n",
    "from segm.model.factory import create_segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOP path: /Users/misha/data/bop\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(os.path.expandvars('$HOME/data'))\n",
    "BOP_PATH = DATA_PATH / 'bop'\n",
    "ITODD_SUBDIR = 'itodd'\n",
    "TRAIN_SUBDIR = 'train_pbr'\n",
    "ITODD_BOP_PATH = BOP_PATH / ITODD_SUBDIR\n",
    "print(f'BOP path: {BOP_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset from /Users/misha/data/bop/itodd/.sdp/v0.0.1\n"
     ]
    }
   ],
   "source": [
    "ds = BopDataset.from_dir(BOP_PATH, ITODD_SUBDIR, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_mask_ratio: 20671 --> 18066\n",
      "min_bbox_dim_ratio: 18066 --> 16071\n"
     ]
    }
   ],
   "source": [
    "aug_name = None\n",
    "img_size = 384\n",
    "# aug_name = AUGNAME_DEFAULT\n",
    "\n",
    "ov = ds.get_objs_view(1, out_size=img_size, aug_name=aug_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environ({'COMMAND_MODE': 'unix2003', 'CONDA_DEFAULT_ENV': 'sdp_py310', 'CONDA_EXE': '/Users/misha/miniconda3/bin/conda', 'CONDA_PREFIX': '/Users/misha/miniconda3/envs/sdp_py310', 'CONDA_PROMPT_MODIFIER': '(sdp_py310) ', 'CONDA_PYTHON_EXE': '/Users/misha/miniconda3/bin/python', 'CONDA_SHLVL': '2', 'HOME': '/Users/misha', 'LOGNAME': 'misha', 'MallocNanoZone': '0', 'OLDPWD': '/', 'ORIGINAL_XDG_CURRENT_DESKTOP': 'undefined', 'PATH': '/Users/misha/miniconda3/envs/sdp_py310/bin:/Users/misha/miniconda3/condabin:/Users/misha/apps/flutter/bin:/usr/local/smlnj/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/Library/Apple/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/misha/Library/Application Support/JetBrains/Toolbox/scripts', 'PWD': '/', 'SHELL': '/bin/zsh', 'SHLVL': '2', 'SSH_AUTH_SOCK': '/private/tmp/com.apple.launchd.repTZKB0MJ/Listeners', 'TMPDIR': '/var/folders/m0/50rptncs5bq6bq7mm6dqzpbr0000gn/T/', 'USER': 'misha', 'VSCODE_AMD_ENTRYPOINT': 'vs/workbench/api/node/extensionHostProcess', 'VSCODE_CODE_CACHE_PATH': '/Users/misha/Library/Application Support/Code/CachedData/6c3e3dba23e8fadc360aed75ce363ba185c49794', 'VSCODE_CRASH_REPORTER_PROCESS_TYPE': 'extensionHost', 'VSCODE_CWD': '/', 'VSCODE_HANDLES_UNCAUGHT_ERRORS': 'true', 'VSCODE_IPC_HOOK': '/Users/misha/Library/Application Support/Code/1.81-main.sock', 'VSCODE_NLS_CONFIG': '{\"locale\":\"en-gb\",\"osLocale\":\"en-gb\",\"availableLanguages\":{},\"_languagePackSupport\":true}', 'VSCODE_PID': '49246', 'XPC_FLAGS': '0x0', 'XPC_SERVICE_NAME': '0', '_': '/Users/misha/miniconda3/envs/sdp_py310/bin/python', '__CFBundleIdentifier': 'com.microsoft.VSCode', '__CF_USER_TEXT_ENCODING': '0x1F5:0x0:0x2', 'ELECTRON_RUN_AS_NODE': '1', 'PYTHONUNBUFFERED': '1', 'PYTHONIOENCODING': 'utf-8', '_CE_CONDA': '', 'CONDA_PREFIX_1': '/Users/misha/miniconda3', 'CONDA_ROOT': '/Users/misha/miniconda3', '_CE_M': '', 'PYTHONPATH': '..:../../lib/segmenter', 'LC_CTYPE': 'UTF-8', 'PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING': '1', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'TERM': 'xterm-color', 'CLICOLOR': '1', 'FORCE_COLOR': '1', 'CLICOLOR_FORCE': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'LD_LIBRARY_PATH': '/Users/misha/miniconda3/envs/sdp_py310/lib/python3.10/site-packages/cv2/../../lib:', 'TORCH_HOME': '/Users/misha/data/torch'})\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "import torch\n",
    "from torch.hub import ENV_TORCH_HOME\n",
    "\n",
    "from timm.models.helpers import load_pretrained, load_custom_pretrained\n",
    "from timm.models.vision_transformer import default_cfgs\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.vision_transformer import _create_vision_transformer\n",
    "\n",
    "from segm.model.vit import VisionTransformer\n",
    "from segm.model.utils import checkpoint_filter_fn\n",
    "from segm.model.decoder import DecoderLinear\n",
    "from segm.model.decoder import MaskTransformer\n",
    "from segm.model.segmenter import Segmenter\n",
    "import segm.utils.torch as ptu\n",
    "\n",
    "if ENV_TORCH_HOME not in os.environ:\n",
    "    os.environ[ENV_TORCH_HOME] = os.path.expandvars('$HOME/data/torch')\n",
    "print(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_vit(model_cfg):\n",
    "    model_cfg = model_cfg.copy()\n",
    "    backbone = model_cfg.pop(\"backbone\")\n",
    "\n",
    "    normalization = model_cfg.pop(\"normalization\")\n",
    "    model_cfg[\"n_cls\"] = 1000\n",
    "    mlp_expansion_ratio = 4\n",
    "    model_cfg[\"d_ff\"] = mlp_expansion_ratio * model_cfg[\"d_model\"]\n",
    "\n",
    "    checkpoint_path = None\n",
    "    if 'checkpoint_path' in model_cfg:\n",
    "        checkpoint_path = model_cfg.pop('checkpoint_path')\n",
    "\n",
    "    if backbone in default_cfgs:\n",
    "        default_cfg = default_cfgs[backbone]\n",
    "        default_cfg = asdict(default_cfg)\n",
    "    else:\n",
    "        default_cfg = dict(\n",
    "            pretrained=False,\n",
    "            num_classes=1000,\n",
    "            drop_rate=0.0,\n",
    "            drop_path_rate=0.0,\n",
    "            drop_block_rate=None,\n",
    "        )\n",
    "    print('default_cfg:', default_cfg)\n",
    "    default_cfg[\"input_size\"] = (\n",
    "        3,\n",
    "        model_cfg[\"image_size\"][0],\n",
    "        model_cfg[\"image_size\"][1],\n",
    "    )\n",
    "    model = VisionTransformer(**model_cfg)\n",
    "\n",
    "    # if checkpoint_path:\n",
    "    #     print(f'Reading encoder weights from {checkpoint_path}')\n",
    "    #     state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    #     filtered_dict = checkpoint_filter_fn(state_dict, model)\n",
    "    #     model.load_state_dict(filtered_dict, strict=True)\n",
    "\n",
    "    if backbone == \"vit_base_patch8_384\":\n",
    "        path = os.path.expandvars(\"$TORCH_HOME/hub/checkpoints/vit_base_patch8_384.pth\")\n",
    "        state_dict = torch.load(path, map_location=\"cpu\")\n",
    "        filtered_dict = checkpoint_filter_fn(state_dict, model)\n",
    "        model.load_state_dict(filtered_dict, strict=True)\n",
    "    elif \"deit\" in backbone:\n",
    "        load_pretrained(model, default_cfg, filter_fn=checkpoint_filter_fn)\n",
    "    else:\n",
    "        print('load_custom_pretrained')\n",
    "        load_custom_pretrained(model, default_cfg)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_decoder(encoder, decoder_cfg):\n",
    "    decoder_cfg = decoder_cfg.copy()\n",
    "    name = decoder_cfg.pop(\"name\")\n",
    "    decoder_cfg[\"d_encoder\"] = encoder.d_model\n",
    "    decoder_cfg[\"patch_size\"] = encoder.patch_size\n",
    "\n",
    "    if \"linear\" in name:\n",
    "        decoder = DecoderLinear(**decoder_cfg)\n",
    "    elif name == \"mask_transformer\":\n",
    "        dim = encoder.d_model\n",
    "        n_heads = dim // 64\n",
    "        decoder_cfg[\"n_heads\"] = n_heads\n",
    "        decoder_cfg[\"d_model\"] = dim\n",
    "        decoder_cfg[\"d_ff\"] = 4 * dim\n",
    "        decoder = MaskTransformer(**decoder_cfg)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown decoder: {name}\")\n",
    "    return decoder\n",
    "\n",
    "\n",
    "def create_segmenter(model_cfg):\n",
    "    model_cfg = model_cfg.copy()\n",
    "    decoder_cfg = model_cfg.pop(\"decoder\")\n",
    "    decoder_cfg[\"n_cls\"] = model_cfg[\"n_cls\"]\n",
    "\n",
    "    encoder = create_vit(model_cfg)\n",
    "\n",
    "    decoder = create_decoder(encoder, decoder_cfg)\n",
    "    model = Segmenter(encoder, decoder, n_cls=model_cfg[\"n_cls\"])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default_cfg: {'tags': deque(['augreg_in21k_ft_in1k']), 'cfgs': {'augreg_in21k_ft_in1k': {'url': 'https://storage.googleapis.com/vit_models/augreg/Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz', 'file': None, 'state_dict': None, 'hf_hub_id': 'timm/', 'hf_hub_filename': None, 'source': None, 'architecture': None, 'tag': None, 'custom_load': True, 'input_size': (3, 384, 384), 'test_input_size': None, 'min_input_size': None, 'fixed_input_size': True, 'interpolation': 'bicubic', 'crop_pct': 1.0, 'test_crop_pct': None, 'crop_mode': 'center', 'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5), 'num_classes': 1000, 'label_offset': None, 'label_names': None, 'label_descriptions': None, 'pool_size': None, 'test_pool_size': None, 'first_conv': 'patch_embed.proj', 'classifier': 'head', 'license': None, 'description': None, 'origin_url': None, 'paper_name': None, 'paper_ids': None, 'notes': None}}, 'is_pretrained': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No pretrained weights exist for this model. Using random initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_custom_pretrained\n",
      "load_from = , pretrained_loc = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Segmenter(\n",
       "  (encoder): VisionTransformer(\n",
       "    (patch_embed): PatchEmbedding(\n",
       "      (proj): Conv2d(9, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): FeedForward(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=192, out_features=1000, bias=True)\n",
       "    (pre_logits): Identity()\n",
       "  )\n",
       "  (decoder): MaskTransformer(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): FeedForward(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mlp): FeedForward(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.100)\n",
       "      )\n",
       "    )\n",
       "    (proj_dec): Linear(in_features=192, out_features=192, bias=True)\n",
       "    (decoder_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    (mask_norm): LayerNorm((9,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cls = 9\n",
    "backbone = 'vit_tiny_patch16_384'\n",
    "checkpoint_path = os.path.expandvars(f'$HOME/data/models/{backbone}.pth')\n",
    "\n",
    "cfg = {\n",
    "    'n_cls': n_cls,\n",
    "    'backbone': backbone,\n",
    "    'checkpoint_path': checkpoint_path,\n",
    "    'image_size': (img_size, img_size),\n",
    "    'channels': 9,\n",
    "    'patch_size': 16,\n",
    "    'd_model': 192,\n",
    "    'n_heads': 3,\n",
    "    'n_layers': 12,\n",
    "    'normalization': 'vit',\n",
    "    'distilled': False,\n",
    "    'decoder': {\n",
    "        'name': 'mask_transformer',\n",
    "        'drop_path_rate': 0.1,\n",
    "        'dropout': 0.1,\n",
    "        'n_layers': 2\n",
    "    }\n",
    "}\n",
    "\n",
    "model = create_segmenter(cfg)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'timm.models._pretrained.DefaultCfg'>\n"
     ]
    }
   ],
   "source": [
    "# print(list(default_cfgs.keys()))\n",
    "dcfg = default_cfgs[backbone]\n",
    "print(type(dcfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tags': deque(['augreg_in21k_ft_in1k']),\n",
       " 'cfgs': {'augreg_in21k_ft_in1k': {'url': 'https://storage.googleapis.com/vit_models/augreg/Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz',\n",
       "   'file': None,\n",
       "   'state_dict': None,\n",
       "   'hf_hub_id': 'timm/',\n",
       "   'hf_hub_filename': None,\n",
       "   'source': None,\n",
       "   'architecture': None,\n",
       "   'tag': None,\n",
       "   'custom_load': True,\n",
       "   'input_size': (3, 384, 384),\n",
       "   'test_input_size': None,\n",
       "   'min_input_size': None,\n",
       "   'fixed_input_size': True,\n",
       "   'interpolation': 'bicubic',\n",
       "   'crop_pct': 1.0,\n",
       "   'test_crop_pct': None,\n",
       "   'crop_mode': 'center',\n",
       "   'mean': (0.5, 0.5, 0.5),\n",
       "   'std': (0.5, 0.5, 0.5),\n",
       "   'num_classes': 1000,\n",
       "   'label_offset': None,\n",
       "   'label_names': None,\n",
       "   'label_descriptions': None,\n",
       "   'pool_size': None,\n",
       "   'test_pool_size': None,\n",
       "   'first_conv': 'patch_embed.proj',\n",
       "   'classifier': 'head',\n",
       "   'license': None,\n",
       "   'description': None,\n",
       "   'origin_url': None,\n",
       "   'paper_name': None,\n",
       "   'paper_ids': None,\n",
       "   'notes': None}},\n",
       " 'is_pretrained': True}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import asdict\n",
    "asdict(dcfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdp_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
